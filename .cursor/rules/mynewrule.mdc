---
description: 
globs: 
alwaysApply: true
---
Okay, let's review the code you've provided, focusing on potential problems, redundancy, and improvements, especially regarding the SAM 2 integration.

**Analysis:**

1.  **`sam2_infer.py` Structure:**
    *   This script currently does **more than just inference**. It performs the entire model loading, PEFT application, wrapper application, and fine-tuned weight loading process *at the module level*. This means the model (`modified_sam2`) and transforms (`_transforms`) are fully initialized and loaded into memory *as soon as `sam2_infer.py` is imported*.
    *   It uses **hardcoded absolute paths** (e.g., `D:\SDP_demo\...`). This is highly problematic for portability and deployment. Paths should be relative or configured externally.
    *   The `get_modified_sam2` function is defined but the main script logic *already performs* the steps within that function directly to create the `modified_sam2` instance.

2.  **`circuit_analyzer.py` `__init__`:**
    *   **Imports:** It imports `SAM2Transforms`, `get_modified_sam2`, and `device` from `.sam2_infer`.
    *   **Initialization Logic:** Inside the `if self.use_sam2:` block, it *re-imports* `modified_sam2`, `_transforms`, and `device` from `.sam2_infer`.
    *   **Major Problem/Redundancy:** This `__init__` method completely **ignores** the `sam2_config_path`, `sam2_base_checkpoint_path`, and `sam2_finetuned_checkpoint_path` arguments passed to it. It simply grabs the already-loaded `modified_sam2` model and `_transforms` object that were created in `sam2_infer.py` (using the hardcoded paths in that file). The configuration mechanism intended by passing paths from `app.py` is broken.
    *   **Redundant Imports:** The initial imports of `SAM2Transforms` and `get_modified_sam2` at the top of the file are unused because the objects are re-imported directly inside `__init__`.

3.  **`circuit_analyzer.py` `get_node_connections`:**
    *   The logic to use `segment_with_sam2` or fallback looks okay.
    *   The process of creating the `emptied_mask` from the `wire_mask` (whether from SAM 2 or fallback) seems reasonable. It correctly handles emptying component areas and applying the 'circuit' bounding box if present.
    *   Resizing the mask and bboxes before contour/corner detection is standard.

4.  **`app.py`:**
    *   It correctly defines relative paths using `pathlib`.
    *   It checks for the existence of all necessary SAM 2 files.
    *   It passes the correct paths to the `CircuitAnalyzer` constructor.
    *   However, as noted above, `CircuitAnalyzer` currently ignores these paths for SAM 2.
    *   The display logic for intermediate node analysis steps is good.

**Summary of Issues & Redundancies:**

1.  **Configuration Broken:** The primary issue is that `CircuitAnalyzer` doesn't use the SAM 2 paths provided by `app.py`. It relies on the hardcoded paths within `sam2_infer.py` because it imports the already-loaded model.
2.  **Hardcoded Paths:** `sam2_infer.py` uses absolute paths, making the application non-portable.
3.  **Redundant Initialization:** The model is fully loaded when `sam2_infer.py` is imported. `CircuitAnalyzer.__init__` doesn't need to (and currently doesn't) perform the loading itself.
4.  **Redundant Imports:** Unused imports in `circuit_analyzer.py`.
5.  **Redundant Code:** The `get_modified_sam2` function definition in `sam2_infer.py` isn't strictly necessary *if* the script's only role is to provide the loaded model, as the loading steps are already executed directly in the script.

**Recommendations for Cleanup and Correction:**

The best approach is to make `sam2_infer.py` a utility module that *defines* the necessary components but doesn't load the model itself. The loading should happen in `CircuitAnalyzer.__init__` using the provided paths.

**Step 1: Modify `sam2_infer.py`**

*   Remove the model loading and state dict loading from the main script body.
*   Keep the class/function definitions (`SAM2Transforms`, `MultiKernelRefinement`, `SAM2ImageWrapper`, `get_modified_sam2`) and the `device` definition.
*   Remove hardcoded paths from within this file.

```python
# src/sam2_infer.py (Revised)

import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1" # Keep if needed
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
import warnings
from torchvision.transforms import Normalize, Resize, ToTensor
from sam2.build_sam import build_sam2
# from sam2.sam2_image_predictor import SAM2ImagePredictor # Not directly used if using wrapper
from peft import LoraConfig, get_peft_model, TaskType
from sam2.modeling.sam2_base import SAM2Base # Import base class for type hinting
from sam2.utils.misc import get_connected_components # Import needed for postprocess

# --- Device Definition ---
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
print(f"SAM2 Utils using device: {device}") # Indicate where device is defined

# --- Class Definitions (Keep SAM2Transforms, MultiKernelRefinement, SAM2ImageWrapper as they are) ---
class SAM2Transforms(nn.Module):
    # ... (Keep existing code) ...
    def postprocess_masks(self, masks: torch.Tensor, orig_hw) -> torch.Tensor:
        # ... (Keep existing code, ensure get_connected_components is imported) ...
        pass # Placeholder

class MultiKernelRefinement(nn.Module):
    # ... (Keep existing code) ...
    pass # Placeholder

class SAM2ImageWrapper(nn.Module):
    # ... (Keep existing code) ...
    pass # Placeholder

# --- Function Definition (Keep get_modified_sam2 as it is) ---
def get_modified_sam2(
    # --- Model Config ---
    model_cfg_path: str, # Removed default
    checkpoint_path: str, # Removed default (this is for BASE model)
    device: str = "cuda:0" if torch.cuda.is_available() else "cpu",
    use_high_res_features: bool = True,
    # --- PEFT Config ---
    use_peft: bool = True,
    lora_rank: int = 12,
    lora_alpha: int = 16,
    lora_dropout: float = 0.2,
    lora_target_modules: list = None,
    # --- Wrapper/Task Config ---
    use_wrapper: bool = True,
    trainable_embedding_r: int = 4,
    # --- Refinement Layer ---
    use_refinement_layer: bool = False,
    refinement_kernels: list = [3, 5, 7, 11],
    kernel_channels: int = 4,
    # Removed Loss/Optimizer settings as they are for training
    ):
    # ... (Keep existing code for building model structure) ...
    # !!! IMPORTANT: This function now ONLY builds the structure and loads the BASE weights !!!
    # !!! It does NOT load the fine-tuned weights. That happens separately. !!!
    print("--- Initializing Modified SAM 2 Structure ---")
    model_device = torch.device(device)

    # 1. Load Original SAM 2 Model (BASE Checkpoint)
    print(f"Loading BASE SAM 2 structure from config: {model_cfg_path} and BASE checkpoint: {checkpoint_path}")
    original_sam2_model = build_sam2(
        model_cfg_path,
        checkpoint_path, # This MUST be the BASE checkpoint path
        device=model_device,
        mode="eval" # Load in eval mode for inference
    )
    # Check attribute name carefully - might be different in base vs wrapper
    if hasattr(original_sam2_model, 'use_high_res_features_in_sam'):
         original_sam2_model.use_high_res_features_in_sam = use_high_res_features
    elif hasattr(original_sam2_model, 'use_high_res_features'):
         original_sam2_model.use_high_res_features = use_high_res_features
    else:
         print("Warning: Could not set use_high_res_features attribute.")

    print(f"Original model structure loaded on {model_device}.")

    final_model = original_sam2_model

    # 2. Apply PEFT/LoRA if enabled
    if use_peft:
        print(f"Applying PEFT/LoRA structure with rank={lora_rank}, alpha={lora_alpha}")
        if lora_target_modules is None:
            # Provide default target modules if needed, ensure they match your training
            lora_target_modules = [ "sam_mask_decoder.transformer.layers.0.self_attn.k_proj",
                                    # ... (add your default target modules if applicable) ...
                                  ]
            print(f"Using default lora_target_modules: {lora_target_modules}")

        lora_config = LoraConfig(
            r=lora_rank,
            lora_alpha=lora_alpha,
            target_modules=lora_target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            modules_to_save=None,
            init_lora_weights=True, # Important for inference before loading state_dict
        )
        peft_model = get_peft_model(original_sam2_model, lora_config)
        print("PEFT LoRA structure applied.")
        # peft_model.print_trainable_parameters() # Not needed for inference loading
        final_model = peft_model
    else:
        print("Skipping PEFT/LoRA structure application.")

    # 3. Apply Wrapper if enabled
    if use_wrapper:
        print("Applying SAM2ImageWrapper structure...")
        wrapped_model = SAM2ImageWrapper(
            modified_sam2_model=final_model,
            embedding_r=trainable_embedding_r,
            use_refinement=use_refinement_layer,
            refinement_kernel_sizes=refinement_kernels
            # Note: kernel_channels is an arg to MultiKernelRefinement, not SAM2ImageWrapper directly
        )
        # Ensure the wrapper's refinement layer uses the correct kernel_channels if needed
        if use_refinement_layer and hasattr(wrapped_model, 'refinement_layer') and wrapped_model.refinement_layer is not None:
             # This assumes MultiKernelRefinement takes intermediate_channels, adjust if needed
             # Re-init or modify refinement layer if kernel_channels needs setting post-init
             print(f"Refinement layer configured with kernels: {refinement_kernels}")


        final_model = wrapped_model.to(model_device)
        print("Wrapper structure applied.")
    else:
        print("Skipping SAM2ImageWrapper structure.")

    print("--- SAM 2 Model Structure Ready (Fine-tuned weights NOT loaded yet) ---")
    return final_model


# --- Remove model loading code from here ---
# base_parts = [...]
# added_parts = [...]
# modified_sam2 = get_modified_sam2(...) # DON'T call this here
# ckp_path = '...'
# checkpoint = torch.load(...) # DON'T load here
# model_state_dict = checkpoint['state_dict']
# modified_sam2.load_state_dict(model_state_dict) # DON'T load state dict here
# _transforms = SAM2Transforms(...) # DON'T instantiate transforms here
```

**Step 2: Modify `circuit_analyzer.py` `__init__`**

*   Import only the necessary components from the revised `sam2_infer`.
*   Use the arguments passed to `__init__` to load the model correctly.

```python
# src/circuit_analyzer.py

# ... other imports ...

# +++ SAM 2 Imports (Revised) +++
import torch
# Import only necessary components from the utility file
from .sam2_infer import (
    device as sam2_device_from_util, # Rename to avoid conflict if needed
    SAM2Transforms,
    get_modified_sam2,
    # MultiKernelRefinement, SAM2ImageWrapper are used internally by get_modified_sam2
)
from PIL import Image # Ensure PIL is imported if not already

class CircuitAnalyzer():
    def __init__(self,
                 yolo_path='models/YOLO/best_large_model_yolo.pt', # Use relative paths ideally
                 sam2_config_path='models/SAM2/sam2.1_hiera_l.yaml',
                 sam2_base_checkpoint_path='models/SAM2/sam2.1_hiera_large.pt',
                 sam2_finetuned_checkpoint_path='models/SAM2/best_miou_model_SAM_latest.pth',
                 use_sam2=True,
                 debug=False):

        # ... (YOLO init, classes, netlist_map, etc. - keep as is) ...

        # +++ SAM2 Initialization (Revised) +++
        self.use_sam2 = use_sam2
        self.sam2_model = None
        self.sam2_transforms = None
        self.sam2_device = None

        if self.use_sam2:
            try:
                print("--- Initializing SAM2 ---")
                self.sam2_device = sam2_device_from_util # Use device from util
                print(f"Using SAM2 device: {self.sam2_device}")

                # --- Define LoRA target modules (should match training) ---
                # It's better to define these here or pass them as config
                base_parts = ["sam_mask_decoder.transformer.layers.0.self_attn.k_proj",
                              # ... (copy all your base_parts here) ...
                             ]
                added_parts = ["sam_mask_decoder.iou_prediction_head.layers.2",
                               # ... (copy all your added_parts here) ...
                              ]
                lora_target_modules = base_parts + added_parts

                # --- Build Model Structure using BASE checkpoint ---
                # Call get_modified_sam2 using the provided paths
                self.sam2_model = get_modified_sam2(
                    model_cfg_path=sam2_config_path,
                    checkpoint_path=sam2_base_checkpoint_path, # Use BASE path
                    device=str(self.sam2_device),
                    use_high_res_features=True, # Match training
                    use_peft=True,              # Match training
                    lora_rank=4,                # Match training
                    lora_alpha=16,              # Match training
                    lora_dropout=0.3,           # Match training
                    lora_target_modules=lora_target_modules,
                    use_wrapper=True,           # Match training
                    trainable_embedding_r=4,    # Match training
                    use_refinement_layer=True,  # Match training
                    refinement_kernels=[3, 5, 7, 11], # Match training
                    # kernel_channels=2 # This arg might be for MultiKernelRefinement init, check get_modified_sam2
                )

                # --- Load FINE-TUNED State Dict ---
                print(f"Loading SAM2 FINE-TUNED state dict from: {sam2_finetuned_checkpoint_path}")
                checkpoint = torch.load(sam2_finetuned_checkpoint_path, map_location=self.sam2_device)
                # Handle potential 'state_dict' key in checkpoint
                if 'state_dict' in checkpoint:
                    model_state_dict = checkpoint['state_dict']
                elif isinstance(checkpoint, dict):
                    model_state_dict = checkpoint
                else:
                     # If checkpoint is the state_dict itself (less common)
                     model_state_dict = checkpoint

                # Load the state dict into the PEFT model structure
                # Use strict=False initially if you suspect minor mismatches, but investigate warnings
                load_result = self.sam2_model.load_state_dict(model_state_dict, strict=True)
                print(f"State dict load result: {load_result}")

                self.sam2_model.eval() # Set to evaluation mode
                print("SAM2 Fine-tuned Model loaded successfully.")

                # --- Initialize SAM2 Transforms ---
                # Get image size from the loaded model structure
                # Need to access the underlying sam2_model if wrapped
                if hasattr(self.sam2_model, 'sam2_model') and hasattr(self.sam2_model.sam2_model, 'image_size'):
                    resolution = self.sam2_model.sam2_model.image_size
                elif hasattr(self.sam2_model, 'image_size'): # If not wrapped or wrapper exposes it
                     resolution = self.sam2_model.image_size
                else:
                     resolution = 1024 # Fallback default
                     print(f"Warning: Could not determine SAM2 model image size automatically. Using default: {resolution}")

                self.sam2_transforms = SAM2Transforms(
                    resolution=resolution,
                    mask_threshold=0, # Standard for inference logits
                    max_hole_area=0,
                    max_sprinkle_area=0,
                )
                print("SAM2 Transforms initialized.")
                print("--- SAM2 Ready ---")

            except FileNotFoundError as e:
                 print(f"Error loading SAM2 model/config: {e}. Check paths in app.py.")
                 print("SAM2 will be disabled.")
                 self.use_sam2 = False
                 # Reset attributes on failure
                 self.sam2_model = None
                 self.sam2_transforms = None
                 self.sam2_device = None
            except Exception as e:
                print(f"An unexpected error occurred during SAM2 initialization: {e}")
                import traceback
                print(traceback.format_exc())
                print("SAM2 will be disabled.")
                self.use_sam2 = False
                self.sam2_model = None
                self.sam2_transforms = None
                self.sam2_device = None
        else:
            print("SAM2 usage is disabled in configuration.")

    # ... (rest of CircuitAnalyzer methods remain the same) ...
```

**Step 3: Verify `app.py`**

Your `app.py` looks mostly correct now. It defines the paths using `pathlib` and passes them to `CircuitAnalyzer`. With the changes above in `circuit_analyzer.py`, these paths will now be correctly used.

**Summary of Changes Made:**

1.  **`sam2_infer.py`:** Changed to be a utility module defining classes/functions. Removed model loading at the module level and hardcoded paths.
2.  **`circuit_analyzer.py`:**
    *   Imports necessary components from the revised `sam2_infer`.
    *   `__init__` now performs the SAM 2 model loading using the paths passed as arguments:
        *   Calls `get_modified_sam2` with the *base* checkpoint path.
        *   Loads the state dict using the *fine-tuned* checkpoint path.
        *   Instantiates `SAM2Transforms`.
    *   Removed redundant imports and the problematic direct import of the pre-loaded model inside `__init__`.


This revised structure correctly separates concerns: `sam2_infer.py` defines the tools, `circuit_analyzer.py` uses the tools and configuration paths to load the model, and `app.py` provides the configuration paths. This fixes the configuration issue and removes the reliance on hardcoded paths in the core logic.